{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0864134",
   "metadata": {},
   "source": [
    "# Bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4e0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b1761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg, FedAdagrad\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import ndarrays_to_parameters, NDArrays, Scalar, Context, parameters_to_ndarrays\n",
    "from flwr.common import Metrics, Context\n",
    "\n",
    "\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "from flwr.client.mod import parameters_size_mod\n",
    "\n",
    "from dmf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d180f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:1\n",
      "Flower 1.10.0 / PyTorch 2.2.1+cu121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haya/miniconda/envs/fedrec/lib/python3.9/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: ashraq/movielens_ratings.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:1\")  \n",
    "NUM_PARTITIONS = 5\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "\n",
    "dataset = \"ashraq/movielens_ratings\" \n",
    "partitioner = IidPartitioner(num_partitions=NUM_PARTITIONS)           \n",
    "fds = FederatedDataset(dataset=dataset,\n",
    "                    partitioners={\"train\": partitioner})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed8c887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Number of Users: 43584\n",
      "Global Number of Movies: 15276\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Process the Federated Dataset & Global Mapping without interaction matrix\n",
    "########################################\n",
    "\n",
    "def compute_global_mapping(fds):\n",
    "    \"\"\"\n",
    "    Compute global mapping for user and item IDs from the full dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    global_train_df = fds.load_split(\"train\").to_pandas()[[\"user_id\", \"movie_id\", \"rating\"]]    # Full training data across all clients\n",
    "    global_test_df = fds.load_split(\"validation\").to_pandas()[[\"user_id\", \"movie_id\", \"rating\"]]    # Full test set\n",
    "\n",
    "    train_users = set(global_train_df['user_id'].unique())\n",
    "    train_movies = set(global_train_df['movie_id'].unique())\n",
    "\n",
    "    global_test_df = global_test_df[\n",
    "        global_test_df['user_id'].isin(train_users) &\n",
    "        global_test_df['movie_id'].isin(train_movies)\n",
    "    ]\n",
    "\n",
    "    all_users = set(global_train_df['user_id']).union(global_test_df['user_id'])\n",
    "    all_movies = set(global_train_df['movie_id']).union(global_test_df['movie_id'])\n",
    "    user_id_map = {user: idx for idx, user in enumerate(sorted(all_users))}\n",
    "    movie_id_map = {movie: idx for idx, movie in enumerate(sorted(all_movies))}\n",
    "\n",
    "    num_users = len(user_id_map)\n",
    "    num_movies = len(movie_id_map)\n",
    "    print(\"Global Number of Users:\", num_users)\n",
    "    print(\"Global Number of Movies:\", num_movies)\n",
    "\n",
    "    return user_id_map, movie_id_map,\n",
    "\n",
    "    \n",
    "# Precompute the global mappings and interaction matrix once.\n",
    "global_user_id_map, global_movie_id_map = compute_global_mapping(fds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bce9fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# The model\n",
    "########################################\n",
    "\n",
    "class MLPLayers(nn.Module):\n",
    "    def __init__(self, sizes, dropout=0.3, activation=\"leaky_relu\", bn=False, init_method=\"norm\", last_activation=True):\n",
    "        super(MLPLayers, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if bn:\n",
    "                layers.append(nn.BatchNorm1d(sizes[i+1]))\n",
    "            if activation == \"leaky_relu\":\n",
    "                layers.append(nn.LeakyReLU())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        if not last_activation:\n",
    "            layers = layers[:-2]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class DMFFederated(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified DMF model that uses learnable embeddings instead of a precomputed global interaction matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_items,\n",
    "                 user_embedding_size=32,\n",
    "                 item_embedding_size=32,\n",
    "                 user_hidden_sizes=[64, 32],\n",
    "                 item_hidden_sizes=[64, 32],\n",
    "                 dropout=0.3,\n",
    "                 activation=\"leaky_relu\",\n",
    "                 bn=False,\n",
    "                 init_method=\"norm\"):\n",
    "        super(DMFFederated, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, user_embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, item_embedding_size)\n",
    "        \n",
    "        self.user_fc_layers = MLPLayers(\n",
    "            [user_embedding_size] + user_hidden_sizes,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            bn=bn,\n",
    "            init_method=init_method,\n",
    "            last_activation=True\n",
    "        )\n",
    "        self.item_fc_layers = MLPLayers(\n",
    "            [item_embedding_size] + item_hidden_sizes,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            bn=bn,\n",
    "            init_method=init_method,\n",
    "            last_activation=True\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.HuberLoss(delta=0.5)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, 0, 0.01)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.0)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, 0, 0.01)\n",
    "    \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_emb = self.user_embedding(user_indices)\n",
    "        item_emb = self.item_embedding(item_indices)\n",
    "        user_features = self.user_fc_layers(user_emb)\n",
    "        item_features = self.item_fc_layers(item_emb)\n",
    "        prediction = torch.mul(user_features, item_features).sum(dim=1)\n",
    "        return prediction\n",
    "    \n",
    "    def calculate_loss(self, batch):\n",
    "        user = batch['user_id']\n",
    "        item = batch['movie_id']\n",
    "        rating = batch['rating']\n",
    "        preds = self.forward(user, item)\n",
    "        loss = self.loss_fn(preds, rating)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, batch):\n",
    "        return self.forward(batch['user_id'], batch['movie_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff09dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Set and get parameters\n",
    "########################################\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "def set_parameters(model, parameters: List[np.ndarray]):\n",
    "    \"\"\"\n",
    "    Sets the parameters of the model using a list of NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model.\n",
    "        parameters (List[np.ndarray]): The model parameters as a list of NumPy arrays.\n",
    "    \"\"\"\n",
    "    params_dict = zip(model.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.from_numpy(v) for k, v in params_dict})\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_parameters(model) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Retrieves the model parameters as a list of NumPy arrays.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: The model parameters as a list of NumPy arrays.\n",
    "    \"\"\"\n",
    "    return [val.cpu().numpy() for _, val in model.state_dict().items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55502702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size is: 7 MB\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Model size\n",
    "########################################\n",
    "\n",
    "model = DMFFederated(\n",
    "            num_users=len(global_user_id_map),\n",
    "            num_items=len(global_movie_id_map),\n",
    "            user_embedding_size=32,\n",
    "            item_embedding_size=32,\n",
    "            user_hidden_sizes=[64, 32],\n",
    "            item_hidden_sizes=[64, 32],\n",
    "            dropout=0.3,\n",
    "            activation=\"leaky_relu\",\n",
    "            bn=False,\n",
    "            init_method=\"norm\"\n",
    "            )\n",
    "\n",
    "# Calculate model size in bytes\n",
    "vals = model.state_dict().values()\n",
    "total_size_bytes = sum(p.element_size() * p.numel() for p in vals)\n",
    "total_size_mb = int(total_size_bytes / (1024**2))\n",
    "\n",
    "print(\"Model size is: {} MB\".format(total_size_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951aa13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Flower Client and Server Function\n",
    "########################################\n",
    "torch.cuda.empty_cache()\n",
    "device = DEVICE \n",
    "num_partitions = NUM_PARTITIONS\n",
    "batch_size = BATCH_SIZE\n",
    "num_epochs = 1\n",
    "lr = 0.0001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "      \n",
    "   \n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.model, parameters)\n",
    "        return get_parameters(self.model), int(1), {}\n",
    "    \n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.model, parameters)\n",
    "        return float(0), int(1), {\"loss\": float(0)}\n",
    "    \n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    return FlowerClient(model).to_client()\n",
    "\n",
    "\n",
    "client_app = ClientApp(\n",
    "    client_fn=client_fn,\n",
    "    mods=[parameters_size_mod]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "bandwidth_sizes = []\n",
    "\n",
    "\n",
    "class BandwidthTrackingFedAvg(FedAvg):\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        # Track sizes of models received\n",
    "        for _, res in results:\n",
    "            ndas = parameters_to_ndarrays(res.parameters)\n",
    "            size = int(sum(n.nbytes for n in ndas) / (1024**2))\n",
    "            print(f\"Server receiving model size: {size} MB\")\n",
    "            bandwidth_sizes.append(size)\n",
    "\n",
    "        return super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "    def configure_fit(self, server_round, parameters, client_manager):\n",
    "        # Call FedAvg for actual configuration\n",
    "        instructions = super().configure_fit(\n",
    "            server_round, parameters, client_manager\n",
    "        )\n",
    "\n",
    "        # Track sizes of models to be sent\n",
    "        for _, ins in instructions:\n",
    "            ndas = parameters_to_ndarrays(ins.parameters)\n",
    "            size = int(sum(n.nbytes for n in ndas) / (1024**2))\n",
    "            print(f\"Server sending model size: {size} MB\")\n",
    "            bandwidth_sizes.append(size)\n",
    "\n",
    "        return instructions\n",
    "    \n",
    "\n",
    "params = ndarrays_to_parameters(get_parameters(model))\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    strategy = BandwidthTrackingFedAvg(\n",
    "        fraction_evaluate=0.0,\n",
    "        initial_parameters=params,\n",
    "    )\n",
    "    config = ServerConfig(num_rounds=1)\n",
    "    return ServerAppComponents(\n",
    "        strategy=strategy,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "backend_config = {\"client_resources\": None}\n",
    "if DEVICE.type == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_gpus\": 2, \"num_cpus\": 1}}\n",
    "\n",
    "server_app = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4cddbee-f017-4ad1-b6e1-2a81b857fcef",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=1, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Evaluating initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server sending model size: 7 MB\n",
      "Server sending model size: 7 MB\n",
      "Server sending model size: 7 MB\n",
      "Server sending model size: 7 MB\n",
      "Server sending model size: 7 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      {'fitins.parameters': {'parameters': 1891904, 'bytes': 7568906}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      Total parameters transmitted: 7568906 bytes\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      {'fitins.parameters': {'parameters': 1891904, 'bytes': 7568906}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      Total parameters transmitted: 7568906 bytes\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      {'fitins.parameters': {'parameters': 1891904, 'bytes': 7568906}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      Total parameters transmitted: 7568906 bytes\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      {'fitins.parameters': {'parameters': 1891904, 'bytes': 7568906}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      Total parameters transmitted: 7568906 bytes\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      {'fitins.parameters': {'parameters': 1891904, 'bytes': 7568906}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=1093841)\u001b[0m \u001b[92mINFO \u001b[0m:      Total parameters transmitted: 7568906 bytes\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 1 round(s) in 7.28s\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server receiving model size: 7 MB\n",
      "Server receiving model size: 7 MB\n",
      "Server receiving model size: 7 MB\n",
      "Server receiving model size: 7 MB\n",
      "Server receiving model size: 7 MB\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Simulation\n",
    "########################################\n",
    "run_simulation(server_app=server_app,\n",
    "               client_app=client_app,\n",
    "               num_supernodes=NUM_PARTITIONS,\n",
    "               backend_config=backend_config\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d49b2f6d-91eb-4161-b86e-74d897cb725a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bandwidth used: 70 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Total bandwidth used for one round: {} MB\".format(sum(bandwidth_sizes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545ba63",
   "metadata": {},
   "source": [
    "For whole process: 70 MB * 5 (clients) * 10 (rounds) * 1 (fraction of selected clients) = 700 MB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
